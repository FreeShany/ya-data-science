{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Read data from the file and prepare a list of words for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'common', 'interactive', 'use', 'of', 'cat', 'for', 'a', 'single', 'file', 'is', 'to', 'output', 'the', 'content', 'of', 'a', 'file', 'to', 'standard', 'output']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "file_obj = open('sentences.txt', 'r')\n",
    "sentences = []\n",
    "for line in file_obj:\n",
    "    list_of_words = re.split('[^a-z]', line.lower())\n",
    "    list_of_words = list(filter(None, list_of_words))\n",
    "    sentences.append(list_of_words)\n",
    "print sentences[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dictionary of all words, used in sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'displays': 155, 'osx': 192, 'selection': 67, 'safari': 221, 'just': 31, 'developed': 194, 'over': 170, 'vermin': 72, 'domestic': 87, 'named': 104, 'installed': 181, 'symbols': 149, 'through': 206, 'human': 51, 'world': 142, 'disk': 244, 'its': 74, 'fifth': 249, 'features': 251, 'tamed': 65, 'upgrade': 232, 'lb': 97, 'drive': 248, 'to': 2, 'won': 239, 'deliberately': 64, 'marks': 226, 'has': 129, 'predecessor': 172, 'non': 199, 'which': 182, 'read': 122, 'october': 160, 'every': 215, 'os': 174, 'they': 68, 'not': 6, 'during': 10, 'now': 195, 'possess': 83, 'intel': 189, 'keyboards': 116, 'bytes': 20, 'unnecessary': 102, 'patch': 217, 'predators': 71, 'small': 60, 'output': 41, 'entirely': 235, 'where': 29, 'ears': 52, 'available': 164, 'on': 115, 'often': 113, 'sequence': 32, 'some': 177, 'lion': 202, 'frequency': 50, 'are': 78, 'year': 216, 'download': 205, 'terms': 123, 'concern': 107, 'error': 106, 'for': 37, 'pipes': 128, 'since': 187, 'factory': 180, 'artificial': 66, 'content': 42, 'version': 163, 'run': 245, 'between': 95, 'new': 148, 'learned': 137, 'three': 218, 'piped': 100, 'common': 34, 'concatenate': 26, 'be': 23, 'weighing': 94, 'genes': 86, 'use': 36, 'standard': 43, 'release': 225, 'diploid': 80, 'members': 90, 'x': 175, 'based': 200, 'safer': 105, 'by': 56, 'both': 79, 'commands': 125, 'installation': 186, 'installs': 236, 'of': 19, 'needing': 121, 'allows': 196, 'according': 167, 'july': 203, 'later': 197, 'mac': 173, 's': 207, 'streams': 18, 'receives': 154, 'successor': 161, 'catenates': 17, 'changes': 9, 'or': 48, 'felis': 92, 'major': 8, 'faint': 47, 'useful': 70, 'apple': 168, 'app': 208, 'community': 193, 'one': 62, 'running': 222, 'unix': 140, 'right': 132, 'simply': 16, 'linux': 141, 'sounds': 45, 'size': 89, 'undergone': 7, 'delete': 119, 'from': 146, 'enhancements': 171, 'second': 227, 'their': 191, 'create': 242, 'people': 63, 'two': 165, 't': 240, 'redirection': 110, 'however': 98, 'cats': 4, 'too': 46, 'basic': 136, 'permanently': 118, 'type': 150, 'dogs': 3, 'store': 209, 'more': 252, 'files': 28, 'releases': 198, 'that': 82, 'started': 139, 'contains': 169, 'releasing': 212, 'tiger': 162, 'released': 159, 'part': 210, 'hear': 44, 'external': 247, 'editions': 166, 'off': 246, 'mice': 57, 'with': 103, 'than': 234, 'those': 54, 'longer': 184, 'count': 253, 'made': 55, 'animals': 61, 'mavericks': 224, 'versions': 213, 'default': 109, 'was': 158, 'single': 38, 'cat': 15, 'will': 30, 'can': 22, 'were': 69, 'wild': 76, 'similar': 88, 'interactive': 35, 'and': 58, 'mountain': 201, 'computers': 178, 'have': 5, 'stdout': 156, 'process': 13, 'lines': 144, 'is': 40, 'received': 145, 'moved': 188, 'it': 21, 'an': 230, 'high': 49, 'as': 14, 'incremental': 231, 'file': 39, 'in': 0, 'need': 241, 'domesticated': 73, 'any': 152, 'domestication': 12, 'if': 99, 'binary': 27, 'processors': 190, 'no': 183, 'rather': 233, 'legibility': 124, 'separate': 243, 'firmware': 179, 'when': 138, 'mid': 176, 'also': 24, 'other': 59, 'arguments': 153, 'adjacent': 114, 'online': 214, 'instead': 112, 'you': 120, 'ancestor': 77, 'offered': 229, 'used': 25, 'chromosomes': 84, 'closest': 75, 'information': 134, 'may': 117, 'symbol': 111, 'leopard': 157, 'update': 250, 'most': 219, 'wrong': 108, 'connected': 127, 'yosemite': 223, 'such': 53, 'comparison': 1, 'recent': 220, 'a': 33, 'purchase': 204, 'genus': 91, 'kg': 96, 'organisms': 81, 'using': 143, 'starting': 126, 'clear': 130, 'stdin': 147, 'flow': 133, 'roughly': 85, 'so': 238, 'switch': 211, 'without': 151, 'command': 135, 'place': 237, 'allow': 185, 'time': 228, 'redirected': 101, 'the': 11, 'typically': 93, 'left': 131}\n"
     ]
    }
   ],
   "source": [
    "unique_words = {}\n",
    "i = 0\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        if not word in unique_words:\n",
    "            unique_words[word] = i\n",
    "            i += 1\n",
    "print unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a matrix where each line is a vector for each sentence with coordinates = how many times each unique word is used in that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.\n",
      "  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  1.  1.\n",
      "  1.  1.  1.  2.  1.  2.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrix_of_sentences = np.zeros((len(sentences), len(unique_words)))\n",
    "i = 0\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        matrix_of_sentences[i, unique_words[word]] += 1\n",
    "    i += 1\n",
    "print matrix_of_sentences[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find dot product for the first sentence and every other sentence. Save indexes of sentences with two minimum dot products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "sentence number: 1dot product: 0.952754440874\n",
      "1: Something closer is found!\n",
      "2\n",
      "sentence number: 2dot product: 0.864473814564\n",
      "1: Something closer is found!\n",
      "3\n",
      "sentence number: 3dot product: 0.895171516328\n",
      "2: Something less closer is found!\n",
      "4\n",
      "sentence number: 4dot product: 0.77708871497\n",
      "1: Something closer is found!\n",
      "5\n",
      "sentence number: 5dot product: 0.940238569533\n",
      "6\n",
      "sentence number: 6dot product: 0.732738758088\n",
      "1: Something closer is found!\n",
      "7\n",
      "sentence number: 7dot product: 0.925875068334\n",
      "8\n",
      "sentence number: 8dot product: 0.884272487528\n",
      "9\n",
      "sentence number: 9dot product: 0.905508881748\n",
      "10\n",
      "sentence number: 10dot product: 0.832816536227\n",
      "11\n",
      "sentence number: 11dot product: 0.880477139067\n",
      "12\n",
      "sentence number: 12dot product: 0.839643254853\n",
      "13\n",
      "sentence number: 13dot product: 0.87035925529\n",
      "14\n",
      "sentence number: 14dot product: 0.87401184233\n",
      "15\n",
      "sentence number: 15dot product: 0.944272178742\n",
      "16\n",
      "sentence number: 16dot product: 0.840636185422\n",
      "17\n",
      "sentence number: 17dot product: 0.956644501524\n",
      "18\n",
      "sentence number: 18dot product: 0.944272178742\n",
      "19\n",
      "sentence number: 19dot product: 0.888544357485\n",
      "20\n",
      "sentence number: 20dot product: 0.842757274492\n",
      "21\n",
      "sentence number: 21dot product: 0.825036446944\n",
      "The closest sentence is: \n",
      "6\n",
      "['domestic', 'cats', 'are', 'similar', 'in', 'size', 'to', 'the', 'other', 'members', 'of', 'the', 'genus', 'felis', 'typically', 'weighing', 'between', 'and', 'kg', 'and', 'lb']\n",
      "Dot product =  0.732738758088\n",
      "The 2nd closest sentence is: \n",
      "4\n",
      "['in', 'one', 'people', 'deliberately', 'tamed', 'cats', 'in', 'a', 'process', 'of', 'artificial', 'selection', 'as', 'they', 'were', 'useful', 'predators', 'of', 'vermin']\n",
      "Dot product =  0.77708871497\n",
      "6 4\n"
     ]
    }
   ],
   "source": [
    "import scipy.spatial as spat\n",
    "\n",
    "min_i_1 = 1\n",
    "min_i_2 = 1\n",
    "min_dot_1 = float(\"inf\")\n",
    "min_dot_2 = float(\"inf\")\n",
    "for i in range(1, len(sentences)):\n",
    "    print i\n",
    "    dot_product = spat.distance.cosine(matrix_of_sentences[0], matrix_of_sentences[i])\n",
    "    print \"sentence number: \" + str(i) + \"dot product: \" + str(dot_product)\n",
    "    if dot_product < min_dot_1:\n",
    "        print \"1: Something closer is found!\"\n",
    "        min_i_2, min_i_1 = min_i_1, i\n",
    "        min_dot_2, min_dot_1 =  min_dot_1, dot_product\n",
    "    elif dot_product < min_dot_2:\n",
    "        print \"2: Something less closer is found!\"\n",
    "        min_i_2 = i\n",
    "        min_dot_2 = dot_product\n",
    "print \"The closest sentence is: \"\n",
    "print min_i_1\n",
    "print  sentences[min_i_1]\n",
    "print \"Dot product =  \" + str(min_dot_1)\n",
    "print \"The 2nd closest sentence is: \"\n",
    "print min_i_2\n",
    "print sentences[min_i_2]\n",
    "print \"Dot product =  \" + str(min_dot_2)\n",
    "\n",
    "result = str(min_i_1) + ' ' + str(min_i_2)\n",
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a result to submission-1.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('submission-1.txt', 'w')\n",
    "f.write(result)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
